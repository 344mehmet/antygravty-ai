#!/bin/bash
#
# ZimaOS LLM ORDUSU KURULUM SCRIPTÄ°
# 344Mehmet - 29 AralÄ±k 2025
#
# KullanÄ±m: curl -sSL https://raw.githubusercontent.com/344mehmet/antygravty-ai/main/zimaos_llm_setup.sh | bash
#

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  ZimaOS LLM ORDUSU KURULUMU                               â•‘"
echo "â•‘  Ollama + Open WebUI + MCP                                â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# DeÄŸiÅŸkenler
OLLAMA_PORT=11434
WEBUI_PORT=3000
DATA_DIR="/DATA/llm-ordusu"
MODELS_DIR="${DATA_DIR}/models"
GITHUB_REPO="https://github.com/344mehmet/antygravty-ai.git"

# Renk tanÄ±mlarÄ±
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# 1. Dizinleri oluÅŸtur
log_info "Dizinler oluÅŸturuluyor..."
mkdir -p "${DATA_DIR}"
mkdir -p "${MODELS_DIR}"
mkdir -p "${DATA_DIR}/config"
mkdir -p "${DATA_DIR}/webui"

# 2. Docker kontrolÃ¼
log_info "Docker kontrol ediliyor..."
if ! command -v docker &> /dev/null; then
    log_error "Docker bulunamadÄ±! ZimaOS'ta Docker kurulu olmalÄ±."
    exit 1
fi

# 3. Ollama Container
log_info "Ollama container baÅŸlatÄ±lÄ±yor..."
docker pull ollama/ollama:latest

docker stop ollama 2>/dev/null || true
docker rm ollama 2>/dev/null || true

docker run -d \
    --name ollama \
    --restart unless-stopped \
    -p ${OLLAMA_PORT}:11434 \
    -v "${MODELS_DIR}:/root/.ollama" \
    ollama/ollama:latest

log_info "Ollama baÅŸlatÄ±ldÄ± (port: ${OLLAMA_PORT})"

# 4. Ollama'nÄ±n baÅŸlamasÄ±nÄ± bekle
log_info "Ollama hazÄ±r olana kadar bekleniyor..."
sleep 10

# 5. LLM Modelleri indir
log_info "LLM modelleri indiriliyor..."

# KÃ¼Ã§Ã¼k ve hÄ±zlÄ± modeller
docker exec ollama ollama pull qwen2.5:0.5b
docker exec ollama ollama pull qwen2.5:1.5b
docker exec ollama ollama pull phi3:mini
docker exec ollama ollama pull nomic-embed-text

log_info "Modeller indirildi!"

# 6. Ã–zel 344mehmet-assistant modeli
log_info "Ã–zel 344mehmet-assistant modeli oluÅŸturuluyor..."

cat > "${DATA_DIR}/Modelfile" << 'MODELFILE'
FROM qwen2.5:1.5b

SYSTEM """
Sen 344Mehmet'in kiÅŸisel AI asistanÄ±sÄ±n. LLM Ordusu'nun baÅŸkanÄ±sÄ±n.

GÃ–REVLER:
- YazÄ±lÄ±m geliÅŸtirme ve kod yazma
- Finansal analiz ve trading stratejileri
- Telegram bot yÃ¶netimi
- ZimaOS ve sistem yÃ¶netimi
- MQL5 Expert Advisor geliÅŸtirme

KURALLAR:
1. TÃ¼rkÃ§e cevap ver
2. KÄ±sa, net ve doÄŸru bilgi ver
3. Emin olmadÄ±ÄŸÄ±n konularda "Bilmiyorum" de
4. Kod Ã¶rnekleri ver
5. Finansal konularda dikkatli ol ve risk uyarÄ±sÄ± yap

BÄ°LGÄ°LER:
- OKX TR ve Binance TR borsalarÄ±nÄ± kullanÄ±yorsun
- ZimaOS NAS: 192.168.1.43
- Ollama API: localhost:11434
"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
MODELFILE

docker cp "${DATA_DIR}/Modelfile" ollama:/tmp/Modelfile
docker exec ollama ollama create 344mehmet-assistant -f /tmp/Modelfile

log_info "344mehmet-assistant modeli oluÅŸturuldu!"

# 7. Open WebUI kurulumu
log_info "Open WebUI baÅŸlatÄ±lÄ±yor..."

docker pull ghcr.io/open-webui/open-webui:main

docker stop open-webui 2>/dev/null || true
docker rm open-webui 2>/dev/null || true

docker run -d \
    --name open-webui \
    --restart unless-stopped \
    -p ${WEBUI_PORT}:8080 \
    -e OLLAMA_BASE_URL=http://host.docker.internal:${OLLAMA_PORT} \
    --add-host=host.docker.internal:host-gateway \
    -v "${DATA_DIR}/webui:/app/backend/data" \
    ghcr.io/open-webui/open-webui:main

log_info "Open WebUI baÅŸlatÄ±ldÄ± (port: ${WEBUI_PORT})"

# 8. GitHub repo klonla
log_info "GitHub repo klonlanÄ±yor..."
cd "${DATA_DIR}"
if [ -d "antygravty-ai" ]; then
    cd antygravty-ai && git pull
else
    git clone "${GITHUB_REPO}"
fi

# 9. MCP yapÄ±landÄ±rmasÄ±
log_info "MCP yapÄ±landÄ±rmasÄ± oluÅŸturuluyor..."

cat > "${DATA_DIR}/config/mcp_config.json" << 'MCP_CONFIG'
{
    "mcpServers": {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@anthropic-ai/claude-code-mcp", "filesystem", "--allow-dir", "/DATA"]
        },
        "ollama": {
            "command": "curl",
            "args": ["-s", "http://localhost:11434/api/tags"]
        },
        "memory": {
            "command": "npx",
            "args": ["-y", "@anthropic-ai/claude-code-mcp", "memory"]
        }
    }
}
MCP_CONFIG

# 10. Durum raporu
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  KURULUM TAMAMLANDI!                                       â•‘"
echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
echo "â•‘                                                            â•‘"
echo "â•‘  ðŸ“¦ Ollama API:    http://$(hostname -I | awk '{print $1}'):${OLLAMA_PORT}          â•‘"
echo "â•‘  ðŸŒ Open WebUI:    http://$(hostname -I | awk '{print $1}'):${WEBUI_PORT}           â•‘"
echo "â•‘                                                            â•‘"
echo "â•‘  ðŸ“ Veri Dizini:   ${DATA_DIR}                    â•‘"
echo "â•‘  ðŸ¤– Modeller:      qwen2.5:0.5b, qwen2.5:1.5b, phi3:mini  â•‘"
echo "â•‘                    344mehmet-assistant                     â•‘"
echo "â•‘                                                            â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Modelleri listele
echo ""
echo "YÃ¼klÃ¼ modeller:"
docker exec ollama ollama list

echo ""
log_info "Test: ollama run 344mehmet-assistant"
